{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in google colab\n",
    "<a href=\"https://colab.research.google.com/github/lmanov1/HeavyMachineryAuction/blob/main/HeavyMachineryAuction.ipynb\" target=\"_blank\">         \n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" \n",
    " target=\"_blank\">\n",
    " title=\"Open this file in Google Colab\" alt=\"Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gdown in /home/lmanov/.local/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: tqdm in /home/lmanov/.local/lib/python3.10/site-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: requests[socks] in /home/lmanov/.local/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (from gdown) (4.10.0)\n",
      "Requirement already satisfied: filelock in /home/lmanov/.local/lib/python3.10/site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lmanov/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lmanov/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lmanov/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lmanov/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/lmanov/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/lmanov/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/lmanov/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/lmanov/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/lmanov/.local/lib/python3.10/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /usr/lib/python3/dist-packages (0.11.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/lmanov/.local/lib/python3.10/site-packages (1.24.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (3.5.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/lmanov/.local/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/lib/python3/dist-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/lmanov/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/lmanov/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/lmanov/.local/lib/python3.10/site-packages (from scikit-learn) (3.4.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pathlib in /home/lmanov/.local/lib/python3.10/site-packages (1.0.1)\n",
      "File 'train.csv' already exists. Skipping download.\n",
      "File 'valid.csv' already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "! pip install gdown\n",
    "! pip install pandas\n",
    "! pip install seaborn\n",
    "! pip install numpy\n",
    "! pip install matplotlib\n",
    "! pip install scikit-learn\n",
    "! pip install pathlib\n",
    "\n",
    "\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pathlib import Path\n",
    "\n",
    "def download_from_gdrive(url, filename):\n",
    "    # Extract the file ID from the URL\n",
    "    file_id = url.split('/')[-2]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Download the file\n",
    "    if Path(filename).exists():\n",
    "        print(f\"File '{filename}' already exists. Skipping download.\")\n",
    "    else:\n",
    "        gdown.download(download_url, filename, quiet=False)\n",
    "        print(f\"File downloaded as: {filename}\")\n",
    "\n",
    "train = 'https://drive.google.com/file/d/1guqSpDv1Q7ZZjSbXMYGbrTvGns0VCyU5/view?usp=drive_link'\n",
    "valid = 'https://drive.google.com/file/d/1j7x8xhMimKbvW62D-XeDfuRyj9ia636q/view?usp=drive_link'\n",
    "# Example usage\n",
    "\n",
    "download_from_gdrive(train, 'train.csv')\n",
    "download_from_gdrive(valid, 'valid.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions attic - not used\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder , Normalizer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.model_selection import train_test_split\n",
    "################################################\n",
    "def scale(df):\n",
    "    # Normalize features in all datasets\n",
    "    scaler = StandardScaler()    \n",
    "    print(\"Scaling on xtrain\")    \n",
    "    return pd.DataFrame(scaler.fit_transform(df))\n",
    "\n",
    "################################################\n",
    "def normalize(df):\n",
    "    # Normalize features in all datasets\n",
    "    normalizer = Normalizer()\n",
    "    print(\"Normalizing on xtrain\")\n",
    "    return pd.DataFrame(normalizer.fit_transform(df))\n",
    "    \n",
    "\n",
    "################################################\n",
    "def fit_pipeline_data(X_train, y_train, df ):   \n",
    "    numeric_features = df.select_dtypes(exclude=['object']).columns\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Pipeline including preprocessing and model training\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', ColumnTransformer(transformers=[\n",
    "            ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('label_encoder', ColumnTransformer(\n",
    "                    transformers=[('label', LabelEncoder(), categorical_features)],\n",
    "                    remainder='passthrough'\n",
    "                ))\n",
    "            ]), categorical_features)\n",
    "        ])),\n",
    "        ('regressor', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)       \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "################################################\n",
    "def hyperparameter_tuning(xtrain, ytrain):\n",
    "    # Initialize the Random Forest Regressor\n",
    "    reg = RandomForestRegressor(n_jobs=8,random_state=43)\n",
    "\n",
    "    # Define hyperparameters to search\n",
    "    param_grid = {\n",
    "        'n_estimators': [42, 100 ,200],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    print(\"Fitting  reg_best on xtrain_scaled, ytrain\" , xtrain.shape, ytrain.shape)\n",
    "\n",
    "    grid_search.fit(xtrain, ytrain)\n",
    "    print(\"Fitted\")\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best params from hyperparametrization: \" , best_params)\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    # best_params = hyperparameter_tuning(xtrain, ytrain)\n",
    "    # reg_best = RandomForestRegressor(**best_params)\n",
    "    # reg_best.fit(xtrain_scaled, ytrain)\n",
    "    # #Make predictions on all datasets\n",
    "    # # 1. Make predictions on the xtrain dataset\n",
    "    # print(\"Make predictions on the xtrain\")\n",
    "    # ytrain_pred = reg_best.predict(xtrain_scaled)\n",
    "    # rmse = RMSE(ytrain_pred, ytrain)    \n",
    "    # print(f\" Best train RMSE: {rmse:.2f} | y_test.std()={ytrain.std():.2f} | y_test.mean()={ytrain.mean():.2f} \")\n",
    "\n",
    "    # # 2. Make predictions on the xtest dataset\n",
    "    # print(\"Make predictions on the xtest and evaluate the model\")\n",
    "    # ytest_pred = reg_best.predict(xtest_scaled)\n",
    "    # rmse = RMSE(ytest_pred, ytest)    \n",
    "    # print(f\" Best test RMSE: {rmse:.2f} | y_test.std()={ytrain.std():.2f} | y_test.mean()={ytrain.mean():.2f} \")\n",
    "\n",
    "    # # 3. Make predictions on the validation dataset\n",
    "    # print(\"Make predictions on the validation dataset\")\n",
    "    # val_pred = reg_best.predict(xval_scaled)\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_462537/604977601.py:89: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  original_dataset = pd.read_csv('train.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(236254, 13) (11573, 13) (101253, 13)\n",
      " Best train RMSE: 6326.16 | ytrain.std()=23568.93 | ytrain.mean()=31948.03 \n",
      " Best test RMSE: 8075.39 | y_test.std()=23568.93 | y_test.mean()=31948.03 \n",
      "Feature importances in descending order: \n",
      " Age                         0.195867\n",
      "fiProductClassDesc          0.153053\n",
      "ProductSize                 0.136461\n",
      "fiSecondaryDesc             0.132678\n",
      "ModelID                     0.100474\n",
      "fiModelDesc                 0.094850\n",
      "YearMade                    0.071663\n",
      "ProductGroupDesc            0.037974\n",
      "SaleYear                    0.036945\n",
      "Tire_Size                   0.021019\n",
      "MachineHoursCurrentMeter    0.010081\n",
      "fiModelSeries               0.007171\n",
      "UsageBand                   0.001765\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# run here\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder , Normalizer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn  as sns\n",
    "import numpy as np\n",
    "\n",
    "################################################\n",
    "def RMSE(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean() ** 0.5\n",
    "\n",
    "################################################\n",
    "def train_random_forest(model , X, y=None, title=\"Training set\"):  \n",
    "\n",
    "    # Make predictions on the training and test sets\n",
    "    y_pred = model.predict(X)\n",
    "    if y is None:\n",
    "        return y_pred\n",
    "    \n",
    "    rmse = RMSE(y_pred, y)    \n",
    "    print(f\" Best {title} RMSE: {rmse:.2f} | y_test.std()={y.std():.2f} | y_test.mean()={y.mean():.2f} \")        \n",
    "    return rmse\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "def preprocess_data(df):    \n",
    "    df2 = df.copy()\n",
    "\n",
    "    columns_to_keep = ['SalesID', 'SalePrice', 'saledate', 'MachineID', 'ModelID','YearMade','MachineHoursCurrentMeter','ProductSize',\\\n",
    "                      'ProductGroupDesc','fiSecondaryDesc','fiModelSeries','fiProductClassDesc','Tire_Size','fiModelDesc','UsageBand']\n",
    "    \n",
    "    # Verification data set has no SalePrice column\n",
    "    if 'SalePrice' not in df2.columns:\n",
    "        columns_to_keep.remove('SalePrice')\n",
    "\n",
    "    # These columns (out of columns_to_keep) have mixed datatypes, so we convert them to strings\n",
    "    df2['fiModelDescriptor'] = df2['fiModelDesc'].astype('str')\n",
    "    df2['fiModelSeries']=df2['fiModelSeries'].astype('str')\n",
    "    df2['Grouser_Tracks']=df2['Grouser_Tracks'].astype('str')\n",
    "    df2['Hydraulics_Flow']=df2['Grouser_Tracks'].astype('str')\n",
    "\n",
    "    # columns_to_keep = ['SalesID', 'SalePrice', 'saledate', 'MachineID', 'ModelID','YearMade','MachineHoursCurrentMeter','ProductSize',\\\n",
    "    #                   'ProductGroupDesc','fiSecondaryDesc','fiModelSeries','fiProductClassDesc','Tire_Size','fiModelDesc',]\n",
    "    \n",
    "    # columns_to_keep3 = ['SalesID', 'SalePrice', 'saledate', 'MachineID', 'ModelID','YearMade','MachineHoursCurrentMeter', \\\n",
    "    #                   'ProductGroupDesc','fiSecondaryDesc','fiModelSeries','fiProductClassDesc','Tire_Size','ProductSize','ProductGroup',\\\n",
    "    #                     'datasource','auctioneerID','Drive_System','Engine_Horsepower']\n",
    "    \n",
    "    # columns_to_keep = ['SalesID', 'SalePrice', 'saledate', 'MachineID', 'ModelID','YearMade','MachineHoursCurrentMeter','ProductSize','ProductGroup',\\\n",
    "    #                    'ProductGroupDesc','fiSecondaryDesc','fiBaseModel','fiModelDesc','UsageBand','state']\n",
    "\n",
    "    # Drop columns that are not in columns_to_keep    \n",
    "    df2=df2[columns_to_keep]\n",
    "    \n",
    "    # handle dates\n",
    "    df2['Saledate'] = pd.to_datetime(df2.saledate)\n",
    "                \n",
    "    # Feature engineering with dates\n",
    "    df2['SaleYear'] =  df2['Saledate'].dt.year    \n",
    "    # Those columns are not very usefull on model\n",
    "    #df2['SaleMonth'] =  df2['Saledate'].dt.month\n",
    "    #df2['SaleDay'] =  df2['Saledate'].dt.day     # sunday sells?  \n",
    "    # Replace values in YearMade column with YearFromSaledate if YearMade is before 1900 or exceeds the sale date\n",
    "    df2.loc[(df2['YearMade'] <= 1900) | (df2['YearMade'] > df2['SaleYear']), 'YearMade'] = df2['SaleYear']   \n",
    "    ##df2['YearMade'] = np.where(df2['YearMade'] < 1991, 1991, df2['YearMade']) \n",
    "    df2['Age'] = df2['Saledate'].dt.year - df2['YearMade']\n",
    "\n",
    "    # machineid is a unique identifier, so we drop it\n",
    "    # sale date is redundant now, so we'll drop it\n",
    "    df2 = df2.drop(columns=['saledate'] , axis=1)\n",
    "    df2 = df2.drop(columns=['Saledate'] , axis=1)\n",
    "    df2 = df2.drop(columns=['MachineID'], axis=1)\n",
    "    \n",
    "    # Finally , set index to SalesID\n",
    "    df2 = df2.set_index('SalesID')    \n",
    "    # df2 = df2.drop(columns=['ModelID'])  # don't touch this one !  \n",
    "    # df2.fillna(df2.mode().iloc[0], inplace=True) - done in the pipeline\n",
    "    return df2\n",
    "\n",
    "\n",
    "# Load the original dataset \n",
    "original_dataset = pd.read_csv('train.csv')\n",
    "# Preprocess the original dataset\n",
    "original_dataset = preprocess_data(original_dataset)\n",
    "original_dataset.drop_duplicates(inplace=True)\n",
    "# Load the validation dataset \n",
    "validation_dataset = pd.read_csv('valid.csv')\n",
    "# Preprocess the validation dataset\n",
    "validation_dataset = preprocess_data(validation_dataset)\n",
    "\n",
    "# Separate features and target from the original dataset\n",
    "X = original_dataset.drop('SalePrice', axis=1)\n",
    "y = original_dataset['SalePrice']\n",
    "\n",
    "#From here########################################################\n",
    "#X_small = X.sample(50000, random_state=43)\n",
    "#X_small = X.head(50000)\n",
    "X_small = X\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X_small, y_small, test_size=0.3, random_state=43)\n",
    "numeric_features = X.select_dtypes(exclude=['object']).columns\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# --------------Impute missing values in the train, test and validation datasets------------------------------\n",
    "mostFreqImputer  = SimpleImputer(strategy='most_frequent')\n",
    "meanImputer  = SimpleImputer(strategy='mean')\n",
    "\n",
    "xtrain[numeric_features]= meanImputer.fit_transform(xtrain[numeric_features])\n",
    "xtrain[categorical_columns]= mostFreqImputer.fit_transform(xtrain[categorical_columns])\n",
    "\n",
    "xtest[numeric_features]= meanImputer.transform(xtest[numeric_features])\n",
    "xtest[categorical_columns]= mostFreqImputer.transform(xtest[categorical_columns])\n",
    "\n",
    "validation_dataset[numeric_features] = meanImputer.transform(validation_dataset[numeric_features])\n",
    "validation_dataset[categorical_columns] = mostFreqImputer.transform(validation_dataset[categorical_columns])\n",
    "\n",
    "# --------------Label encoding on the train, test and validation datasets------------------------------\n",
    "#Combine train , test and validation datasets for label encoding to keep label consistency between'em\n",
    "combined_dataset = pd.concat([xtrain, validation_dataset, xtest], ignore_index=True)\n",
    "# Label encode categorical features\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    combined_dataset[col] = le.fit_transform(combined_dataset[col].astype(str))    \n",
    "# Split back into originaltrain, test and validation datasets\n",
    "xtrain = combined_dataset.iloc[:len(xtrain)]\n",
    "validation_dataset_encoded = combined_dataset.iloc[len(xtrain):len(validation_dataset)+len(xtrain)]\n",
    "xtest = combined_dataset.iloc[len(xtrain)+ len(validation_dataset):]\n",
    "print(xtrain.shape, validation_dataset.shape, xtest.shape)\n",
    "xtest.to_csv(f'X_test.csv')\n",
    "xtrain.to_csv(f'X_train.csv')\n",
    "\n",
    "# ------------FROM HERE Train model and predict on different data sets  -------------------------\n",
    "# Train a random forest regressor - 20, 10, 200 derived from last hyperparameters tuning (uncomment the hyperparameter_tuning below if needed)\n",
    "model = RandomForestRegressor(random_state=43, max_depth=20, min_samples_split=10 , n_estimators=200)\n",
    "# 1. Fit model and make predictions on the xtrain dataset\n",
    "model.fit(xtrain, ytrain)\n",
    "ytrain_pred = model.predict(xtrain)   \n",
    "print(f\" Best train RMSE: {RMSE(ytrain_pred, ytrain):.2f} | ytrain.std()={ytrain.std():.2f} | ytrain.mean()={ytrain.mean():.2f} \")  \n",
    "#2. Make predictions on the xtest dataset\n",
    "ytest_pred = model.predict(xtest)\n",
    "print(f\" Best test RMSE: {RMSE(ytest_pred, ytest):.2f} | y_test.std()={ytrain.std():.2f} | y_test.mean()={ytrain.mean():.2f} \")  \n",
    "#Print feature importances , in descending order\n",
    "print(\"Feature importances in descending order: \\n\", pd.Series(\n",
    "    model.feature_importances_,\n",
    "    index=model.feature_names_in_\n",
    ").sort_values(ascending=False))\n",
    "\n",
    "# Make predictions on the validation dataset\n",
    "val_pred = model.predict(validation_dataset_encoded)\n",
    "# ------------ TO  HERE -------------------------\n",
    "\n",
    "# ------------ FROM HERE Determine bes parameters set with grid search , than train regressor using the above set ,and predict on different data sets FROM HERE -------------------------\n",
    "# Train the model with the best hyperparameters\n",
    "# best_params = hyperparameter_tuning(xtrain, ytrain)\n",
    "# #Best params from hyperparametrization:  {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "# reg_best = RandomForestRegressor(**best_params)\n",
    "# # Fit the model on the training data\n",
    "# reg_best.fit(xtrain, ytrain)\n",
    "\n",
    "# #With the best parameters , make predictions on all datasets\n",
    "# # 1. Make predictions on the xtrain dataset\n",
    "# print(\"Make predictions on the xtrain\")\n",
    "# ytrain_pred = reg_best.predict(xtrain)\n",
    "# rmse = RMSE(ytrain_pred, ytrain)    \n",
    "# print(f\" Best train RMSE: {rmse:.2f} | y_test.std()={ytrain.std():.2f} | y_test.mean()={ytrain.mean():.2f} \")\n",
    "\n",
    "# # 2. Make predictions on the xtest dataset\n",
    "# print(\"Make predictions on the xtest and evaluate the model\")\n",
    "# ytest_pred = reg_best.predict(xtest)\n",
    "# rmse = RMSE(ytest_pred, ytest)    \n",
    "# print(f\" Best test RMSE: {rmse:.2f} | y_test.std()={ytrain.std():.2f} | y_test.mean()={ytrain.mean():.2f} \")\n",
    "\n",
    "# # 3. Make predictions on the validation dataset\n",
    "# print(\"Make predictions on the validation dataset\")\n",
    "# val_pred = reg_best.predict(validation_dataset_encoded)\n",
    "\n",
    "# # Print feature importances , in descending order\n",
    "# print(\"Feature importances in descending order: \\n\", pd.Series(\n",
    "#     reg_best.feature_importances_,\n",
    "#     index=reg_best.feature_names_in_\n",
    "# ).sort_values(ascending=False))\n",
    "# ------------ TO  HERE -------------------------\n",
    "\n",
    "# Construct a series with the predictions and the SalesID as the index for submission\n",
    "val_pred = pd.Series(val_pred, index=validation_dataset.index, name='SalePrice')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     11573.000000\n",
      "mean      32940.254481\n",
      "std       23218.857310\n",
      "min        5590.372130\n",
      "25%       14872.462266\n",
      "50%       25731.441718\n",
      "75%       44480.814682\n",
      "max      135870.498855\n",
      "Name: SalePrice, dtype: float64\n",
      "SalesID\n",
      "1222837    63450.385027\n",
      "1222839    61389.226189\n",
      "1222841    39192.134014\n",
      "1222843    17161.152563\n",
      "1222845    44868.006749\n",
      "1222847    12346.061585\n",
      "1222849    21930.522770\n",
      "1222850    27832.525942\n",
      "1222855    76650.991593\n",
      "1222863    25515.970810\n",
      "Name: SalePrice, dtype: float64\n",
      "(11573, 13)\n"
     ]
    }
   ],
   "source": [
    "print(val_pred.describe())\n",
    "print(val_pred.head(10))\n",
    "print(validation_dataset.shape)\n",
    "\n",
    "# create file for submission\n",
    "from datetime import datetime\n",
    "f'submission_{datetime.now().isoformat()}'\n",
    "val_pred.to_csv(f'submission_{datetime.now().isoformat()}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
