{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in google colab\n",
    "<a href=\"https://colab.research.google.com/github/lmanov1/HeavyMachineryAuction/blob/main/HeavyMachineryAuction.ipynb\" target=\"_blank\">         \n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" \n",
    " target=\"_blank\">\n",
    " title=\"Open this file in Google Colab\" alt=\"Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gdown\n",
    "! pip install pandas\n",
    "! pip install seaborn\n",
    "! pip install numpy\n",
    "! pip install matplotlib\n",
    "! pip install scikit-learn\n",
    "! pip install pathlib\n",
    "\n",
    "\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pathlib import Path\n",
    "\n",
    "def download_from_gdrive(url, filename):\n",
    "    # Extract the file ID from the URL\n",
    "    file_id = url.split('/')[-2]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Download the file\n",
    "    if Path(filename).exists():\n",
    "        print(f\"File '{filename}' already exists. Skipping download.\")\n",
    "    else:\n",
    "        gdown.download(download_url, filename, quiet=False)\n",
    "        print(f\"File downloaded as: {filename}\")\n",
    "\n",
    "train = 'https://drive.google.com/file/d/1guqSpDv1Q7ZZjSbXMYGbrTvGns0VCyU5/view?usp=drive_link'\n",
    "valid = 'https://drive.google.com/file/d/1j7x8xhMimKbvW62D-XeDfuRyj9ia636q/view?usp=drive_link'\n",
    "# Example usage\n",
    "\n",
    "download_from_gdrive(train, 'train.csv')\n",
    "download_from_gdrive(valid, 'valid.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('train.csv')\n",
    "#data.info()\n",
    "#print(data['fiModelSeries'])\n",
    "data['fiModelDescriptor'] = data['fiModelDesc'].astype('str')\n",
    "data['fiModelSeries']=data['fiModelSeries'].astype('str')\n",
    "data['Grouser_Tracks']=data['Grouser_Tracks'].astype('str')\n",
    "data['Hydraulics_Flow']=data['Grouser_Tracks'].astype('str')\n",
    "\n",
    "#data['fiModelSeries'].to_csv('fiModelSeries.csv')\n",
    "# # Split into features and target\n",
    "X = data.drop('SalePrice', axis=1)  # Assuming 'SalePrice' is the target\n",
    "y = data['SalePrice']\n",
    "\n",
    "X_small = X.sample(20000)\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "# X_small = X\n",
    "# y_small = y.loc[X.index]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, test_size=0.2, random_state=42)\n",
    "#X_train.info()\n",
    "\n",
    "# # Identify numerical and categorical features\n",
    "numerical_features = X_train.select_dtypes(include=['float', 'int','bool']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# print(\"--------------------------------------\")\n",
    "# print(numerical_features)\n",
    "# for col in numerical_features:\n",
    "#    print(col, X_train[col].dtype)\n",
    "#    X_train[col].astype('float')\n",
    "# print(\"--------------------------------------\")\n",
    "# print(categorical_features)\n",
    "# for col in categorical_features:\n",
    "#    print(col, X_train[col].dtype)\n",
    "#    X_train[col].astype('str')\n",
    "print(\"--------------------------------------\")\n",
    "# # Create transformers for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# # Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# # Create a pipeline for model training\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestRegressor(random_state=42))  # Replace with your desired model\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def manipulate_data(df):\n",
    "    # Make a copy of the dataframe to avoid modifying the original data\n",
    "    df2 = df.copy()    \n",
    "    # df2['UsageBand'] = df2['UsageBand'].astype('category')\n",
    "    # df2 = pd.get_dummies(df2, columns=['UsageBand'], prefix='UsageBand')\n",
    "    \n",
    "    # handle dates in processing\n",
    "    df2['Saledate'] = pd.to_datetime(df.saledate)\n",
    "    #df2 = df2.select_dtypes(['number', 'datetime','bool'])  # drop all categorical variables\n",
    "            \n",
    "    # Feature engineering with dates\n",
    "    df2['SaleYear'] =  df2['Saledate'].dt.year\n",
    "    df2['SaleMonth'] =  df2['Saledate'].dt.month     \n",
    "    # Replace values in YearMade column with YearFromSaledate if YearMade is before 1900 or exceeds the sale date\n",
    "    df2.loc[(df2['YearMade'] <= 1900) | (df2['YearMade'] > df2['SaleYear']), 'YearMade'] = df2['SaleYear']    \n",
    "    df2['Age'] = df2['Saledate'].dt.year - df2['YearMade']\n",
    "    \n",
    "    # for col in [\"YearMade\", \"Age\",\"SaleYear\",\"SaleMonth\"]:\n",
    "    #     print(f\"df2[{col}]: max {df2[col].max():.2f} , min {df2[col].min():.2f} , mean {df2[col].mean():.2f} , std {df2[col].std():.2f},  median {df2[col].median():.2f}\")\n",
    "\n",
    "    # machineid is a unique identifier, so we drop it\n",
    "    # sale date is redundant now, so we'll drop it\n",
    "    df2 = df2.drop(columns=['saledate'])\n",
    "    df2 = df2.drop(columns=['Saledate'])\n",
    "    df2 = df2.drop(columns=['MachineID'])\n",
    "    # df2 = df2.drop(columns=['ModelID'])  # don't touch this one !\n",
    "\n",
    "    df2 = df2.set_index('SalesID')  # set the index to the unique identifier\n",
    "    \n",
    "    for col in df2.select_dtypes(include=[np.number,np.datetime64]).columns:    \n",
    "        mean = df2[col].mean()\n",
    "        df2[col] = df2[col].fillna(mean) # fill missing values with mean         \n",
    "\n",
    "    # for col in df2.select_dtypes(include=[np.number,np.datetime64]).columns:   # apply filtering only to numerical columns\n",
    "    #     if col not in ['SalesID', 'SalePrice']:\n",
    "    #         mean = df2[col].mean()\n",
    "    #         std_dev = df2[col].std()\n",
    "    #         median = df2[col].median()\n",
    "    #         lower_bound = mean - 3 * std_dev\n",
    "    #         upper_bound = mean + 3 * std_dev\n",
    "\n",
    "    #         print(f\" percent of outliers in {col}: { ((df2[col] < lower_bound) | (df2[col] >= upper_bound)).sum()/df2[col].shape[0]*100:.2f},  min {df2[col].min():.2f}, max {df2[col].max():.2f} , mean {mean:.2f} , std {std_dev:.2f},  median {median:.2f}\")\n",
    "    #         # Normalize the values in place\n",
    "    #         df2[col] = df2[col].mask(((df2[col] < lower_bound) | (df2[col] > upper_bound)), median)    \n",
    "    # print (\"After 6 sigma filtering \", df2.shape )   \n",
    "        \n",
    "    return df2\n",
    "\n",
    "\n",
    "### MAIN #########\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "# Clean data\n",
    "print (\"Initial \", df.shape)\n",
    "print(\"-----------------------------------------------------\")\n",
    "df2 = manipulate_data(df)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print (\"After manipulate_data \", df2.shape )\n",
    "print(\"-----------------------------------------------------\")\n",
    "df2 = df2.drop_duplicates()  # drop duplicates \n",
    "print(\"After drop_duplicates \", df2.shape)\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "#print(\"before 6 sigma \", df2.shape )\n",
    "# #Remove outliers using 6 sigma method\n",
    "#for col in df2.select_dtypes(include=[np.number,np.datetime64]).columns:    \n",
    "#    mean = df2[col].mean()\n",
    "#    std_dev = df2[col].std()    \n",
    "#    lower_bound = mean - 3 * std_dev\n",
    "#    upper_bound = mean + 3 * std_dev\n",
    "#    df2 = df2[(df2[col] >= lower_bound) & (df2[col] <= upper_bound)]\n",
    "\n",
    "#print(\"After 6 sigma \", df2.shape )\n",
    "print(df2.head()  )  \n",
    "\n",
    "# Prepare data for training\n",
    "X = df2.drop(columns='SalePrice')\n",
    "y = df2['SalePrice']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "MAX_DEPTH = 20\n",
    "ESTIMATORS=170\n",
    "TEST_SIZE=0.2\n",
    "\n",
    "###################################################\n",
    "def RMSE(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean() ** 0.5\n",
    "\n",
    "\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "X_small = X.sample(20000)\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "# X_small = X\n",
    "# y_small = y.loc[X_small.index]\n",
    "\n",
    "model = RandomForestRegressor(random_state=RANDOM_STATE, max_depth=MAX_DEPTH)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "# Dropping columns\n",
    "for col in X_train.columns:\n",
    "   if X_train[col].nunique() == 1:\n",
    "        print(\"Dropping \", col)\n",
    "        X_train.drop(col,inplace=True,axis=1)  \n",
    "\n",
    "# Perform one-hot encoding on the train dataset \n",
    "cols_For_onehot_encoding = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "#print(\"cols_For_onehot_encoding\",  cols_For_onehot_encoding , len(cols_For_onehot_encoding))\n",
    "to_drop = ['fiModelDesc','fiBaseModel','fiSecondaryDesc','fiModelSeries','fiModelDescriptor','fiProductClassDesc','ProductGroupDesc','state']\n",
    "\n",
    "for column in to_drop:\n",
    "    X_train = X_train.drop(column   , axis=1)    \n",
    "    cols_For_onehot_encoding = cols_For_onehot_encoding.drop(column)\n",
    "print(\"cols_For_onehot_encoding shortened\" ,cols_For_onehot_encoding, len(cols_For_onehot_encoding))\n",
    "\n",
    "print(\"Before=====\",  X_train.shape ,X_test.shape )\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=cols_For_onehot_encoding , drop_first=True)\n",
    "# Align the test dataset with the train dataset columns\n",
    "X_test = pd.get_dummies(X_test, columns=cols_For_onehot_encoding, drop_first=True)\n",
    "\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(\"After=====\",  X_train.shape ,X_test.shape )\n",
    "\n",
    "X_train.info()\n",
    "X_train.head()\n",
    "X_test.to_csv(f'X_test.csv')\n",
    "X_train.to_csv(f'X_train.csv')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "test_rmse = RMSE(y_test_pred, y_test)\n",
    "train_rmse = RMSE(y_train_pred, y_train)\n",
    "\n",
    "print(f\" Best test RMSE: {test_rmse:.2f} | y_test.std()={y_test.std():.2f} | y_test.mean()={y_test.mean():.2f} \")\n",
    "print(f\" Best train RMSE: {train_rmse:.2f} | y_train.std()={y_train.std():.2f} | y_train.mean()={y_train.mean():.2f}\")\n",
    "\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1) \n",
    "# feature_importances = pd.DataFrame(result.importances_mean, index=X_test.columns, columns=['Permutation Importance'])\n",
    "# feature_importances['Permutation Importance'] = feature_importances['Permutation Importance'] / feature_importances['Permutation Importance'].sum()\n",
    "# feature_importances['Feature importance'] = model.feature_importances_\n",
    "# feature_importances = feature_importances.sort_values(by='Permutation Importance', ascending=False)\n",
    "# print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "X_valid = pd.read_csv('valid.csv')\n",
    "print (\"Initial \", X_valid.shape)\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "X_valid = manipulate_data(X_valid)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print (\"After manipulate_data \", X_valid.shape )\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "# Align the test dataset with the train dataset columns\n",
    "#X_valid = pd.get_dummies(X_valid, columns=['datasource'], prefix='datasource' , drop_first=True)\n",
    "X_valid = pd.get_dummies(X_valid, columns=cols_For_onehot_encoding , drop_first=True)\n",
    "X_valid = X_valid.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print (\"After one hot encoding and reindex \", X_valid.shape )\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "y_valid_pred = model.predict(X_valid)  # predict on the validation set             \n",
    "y_valid_pred = pd.Series(y_valid_pred, index=X_valid.index, name='SalePrice')\n",
    "y_valid_pred.info()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "f'submission_{datetime.now().isoformat()}'\n",
    "y_valid_pred.to_csv(f'submission_{datetime.now().isoformat()}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# #print(df2.corr())\n",
    "# # Create a figure with a custom size\n",
    "# plt.figure(figsize=(12, 10))  # Adjust figsize to fit your needs\n",
    "# sns.heatmap(X_valid.corr(), annot=True, fmt='0.2f')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Predict and analyse the results\n",
    "# #Compute permutation feature importance\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ###################################################\n",
    "# def important_features_analysis(model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1):\n",
    "#     result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)    \n",
    "\n",
    "#     # Summarize feature importance    \n",
    "#     feature_importances = pd.DataFrame(result.importances_mean, index=X_test.columns, columns=['Permutation Importance'])\n",
    "#     feature_importances['Permutation Importance'] = feature_importances['Permutation Importance'] / feature_importances['Permutation Importance'].sum()\n",
    "#     feature_importances['Feature importance'] = model.feature_importances_\n",
    "#     feature_importances = feature_importances.sort_values(by='Permutation Importance', ascending=False)\n",
    "#     print(feature_importances)\n",
    "\n",
    "# ###################################################\n",
    "# def analyse_prediction(model , X_data, y_data , y_pred, random_state=RANDOM_STATE):\n",
    "    \n",
    "#     print(\"\\n\\nRMSE:\", round(RMSE(y_pred, y_data),2))\n",
    "#     print(\"STD\", round(y_data.std(),2))\n",
    "#     important_features_analysis(model, X_data, y_data)\n",
    "   \n",
    "\n",
    "\n",
    "# def plot_prediction(y_data, y_pred, title='Actual vs Predicted Prices'):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.scatterplot(x=y_data, y=y_pred)\n",
    "#     xx = np.linspace(y_data.min(), y_data.max(), 100)\n",
    "#     plt.plot(xx, xx, 'r--')\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('actual')\n",
    "#     plt.ylabel('predicted')\n",
    "#     plt.show()\n",
    "\n",
    "# ###################################################\n",
    "# def predict(model,X_data , y_data=None, random_state=RANDOM_STATE):\n",
    "#     y_pred = model.predict(X_data)\n",
    "#     if y_data is None:\n",
    "#         return y_pred\n",
    "    \n",
    "#     analyse_prediction(model, X_data , y_data , y_pred, random_state=random_state)    \n",
    "#     return y_pred\n",
    "# ###################################################\n",
    "# def summary(model,X_train, X_test, y_train, y_test, random_state=RANDOM_STATE):\n",
    "    \n",
    "#     y_train_pred = predict(model,X_train, y_train,random_state=random_state)\n",
    "#     y_test_pred = predict(model,X_test, y_test,random_state=random_state)   \n",
    "    \n",
    "#     plot_prediction(y_train, y_train_pred,\"Actual vs Predicted Prices (Train)\")\n",
    "     \n",
    "#     plot_prediction(y_test, y_test_pred,\"Actual vs Predicted Prices (Test)\")\n",
    "   \n",
    "#     return y_train_pred, y_test_pred\n",
    "\n",
    "# ###################################################\n",
    "# def summary2(model,X_train, X_test, y_train, y_test, random_state=RANDOM_STATE):\n",
    "    \n",
    "#     y_train_pred, y_test_pred = summary(model, X_train, X_test, y_train, y_test, random_state=random_state)\n",
    "#     return y_train_pred, y_test_pred\n",
    "    \n",
    "# ###################################################\n",
    "# def plot_residuals(y_true, y_pred, title):\n",
    "#     residuals = y_true - y_pred\n",
    "#     plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Predicted Values')\n",
    "#     plt.ylabel('Residuals')\n",
    "#     plt.axhline(y=0, color='r', linestyle='--')\n",
    "#     plt.show()\n",
    "\n",
    "# ###############################################################\n",
    "# # Assuming the summary function is defined as in the provided code snippet\n",
    "# def summary_with_residuals(model, X_train, X_test, y_train, y_test, random_state):\n",
    "#     y_train_pred, y_test_pred = summary(model, X_train, X_test, y_train, y_test, random_state=random_state)\n",
    "    \n",
    "#     # Plotting residuals for training data\n",
    "#     plot_residuals(y_train, y_train_pred, \"Residuals for Training Data\")\n",
    "    \n",
    "#     # Plotting residuals for test data\n",
    "#     plot_residuals(y_test, y_test_pred, \"Residuals for Test Data\")\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------------------------------------\n",
    "# # Call the modified summary function with residuals plotting\n",
    "# summary_with_residuals(model, X_train, X_test, y_train, y_test, random_state=RANDOM_STATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
