{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in google colab\n",
    "<a href=\"https://colab.research.google.com/github/lmanov1/HeavyMachineryAuction/blob/main/HeavyMachineryAuction.ipynb\" target=\"_blank\">         \n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" \n",
    " target=\"_blank\">\n",
    " title=\"Open this file in Google Colab\" alt=\"Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gdown\n",
    "! pip install pandas\n",
    "! pip install seaborn\n",
    "! pip install numpy\n",
    "! pip install matplotlib\n",
    "! pip install scikit-learn\n",
    "! pip install pathlib\n",
    "\n",
    "\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pathlib import Path\n",
    "\n",
    "def download_from_gdrive(url, filename):\n",
    "    # Extract the file ID from the URL\n",
    "    file_id = url.split('/')[-2]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "    # Download the file\n",
    "    if Path(filename).exists():\n",
    "        print(f\"File '{filename}' already exists. Skipping download.\")\n",
    "    else:\n",
    "        gdown.download(download_url, filename, quiet=False)\n",
    "        print(f\"File downloaded as: {filename}\")\n",
    "\n",
    "train = 'https://drive.google.com/file/d/1guqSpDv1Q7ZZjSbXMYGbrTvGns0VCyU5/view?usp=drive_link'\n",
    "valid = 'https://drive.google.com/file/d/1j7x8xhMimKbvW62D-XeDfuRyj9ia636q/view?usp=drive_link'\n",
    "# Example usage\n",
    "\n",
    "download_from_gdrive(train, 'train.csv')\n",
    "download_from_gdrive(valid, 'valid.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_903512/1287893747.py:58: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('train.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial  (401125, 53)\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "After manipulate_data  (401125, 53)\n",
      "-----------------------------------------------------\n",
      "After drop_duplicates  (385107, 53)\n",
      "-----------------------------------------------------\n",
      "         SalePrice  ModelID  datasource  auctioneerID  YearMade  \\\n",
      "SalesID                                                           \n",
      "1139246      66000     3157         121           3.0      2004   \n",
      "1139248      57000       77         121           3.0      1996   \n",
      "1139249      10000     7009         121           3.0      2001   \n",
      "1139251      38500      332         121           3.0      2001   \n",
      "1139253      11000    17311         121           3.0      2007   \n",
      "\n",
      "         MachineHoursCurrentMeter UsageBand fiModelDesc fiBaseModel  \\\n",
      "SalesID                                                               \n",
      "1139246                      68.0       Low        521D         521   \n",
      "1139248                    4640.0       Low      950FII         950   \n",
      "1139249                    2838.0      High         226         226   \n",
      "1139251                    3486.0      High    PC120-6E       PC120   \n",
      "1139253                     722.0    Medium        S175        S175   \n",
      "\n",
      "        fiSecondaryDesc  ... Pattern_Changer Grouser_Type Backhoe_Mounting  \\\n",
      "SalesID                  ...                                                 \n",
      "1139246               D  ...             NaN          NaN              NaN   \n",
      "1139248               F  ...             NaN          NaN              NaN   \n",
      "1139249             NaN  ...             NaN          NaN              NaN   \n",
      "1139251             NaN  ...             NaN          NaN              NaN   \n",
      "1139253             NaN  ...             NaN          NaN              NaN   \n",
      "\n",
      "        Blade_Type Travel_Controls Differential_Type Steering_Controls  \\\n",
      "SalesID                                                                  \n",
      "1139246        NaN             NaN          Standard      Conventional   \n",
      "1139248        NaN             NaN          Standard      Conventional   \n",
      "1139249        NaN             NaN               NaN               NaN   \n",
      "1139251        NaN             NaN               NaN               NaN   \n",
      "1139253        NaN             NaN               NaN               NaN   \n",
      "\n",
      "        SaleYear SaleMonth Age  \n",
      "SalesID                         \n",
      "1139246     2006        11   2  \n",
      "1139248     2004         3   8  \n",
      "1139249     2004         2   3  \n",
      "1139251     2011         5  10  \n",
      "1139253     2009         7   2  \n",
      "\n",
      "[5 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def manipulate_data(df):\n",
    "    # Make a copy of the dataframe to avoid modifying the original data\n",
    "    df2 = df.copy()    \n",
    "    # df2['UsageBand'] = df2['UsageBand'].astype('category')\n",
    "    # df2 = pd.get_dummies(df2, columns=['UsageBand'], prefix='UsageBand')\n",
    "    \n",
    "    # handle dates in processing\n",
    "    df2['Saledate'] = pd.to_datetime(df.saledate)\n",
    "    #df2 = df2.select_dtypes(['number', 'datetime','bool'])  # drop all categorical variables\n",
    "            \n",
    "    # Feature engineering with dates\n",
    "    df2['SaleYear'] =  df2['Saledate'].dt.year\n",
    "    df2['SaleMonth'] =  df2['Saledate'].dt.month     \n",
    "    # Replace values in YearMade column with YearFromSaledate if YearMade is before 1900 or exceeds the sale date\n",
    "    df2.loc[(df2['YearMade'] <= 1900) | (df2['YearMade'] > df2['SaleYear']), 'YearMade'] = df2['SaleYear']    \n",
    "    df2['Age'] = df2['Saledate'].dt.year - df2['YearMade']\n",
    "    \n",
    "    # for col in [\"YearMade\", \"Age\",\"SaleYear\",\"SaleMonth\"]:\n",
    "    #     print(f\"df2[{col}]: max {df2[col].max():.2f} , min {df2[col].min():.2f} , mean {df2[col].mean():.2f} , std {df2[col].std():.2f},  median {df2[col].median():.2f}\")\n",
    "\n",
    "    # machineid is a unique identifier, so we drop it\n",
    "    # sale date is redundant now, so we'll drop it\n",
    "    df2 = df2.drop(columns=['saledate'])\n",
    "    df2 = df2.drop(columns=['Saledate'])\n",
    "    df2 = df2.drop(columns=['MachineID'])\n",
    "    # df2 = df2.drop(columns=['ModelID'])  # don't touch this one !\n",
    "\n",
    "    df2 = df2.set_index('SalesID')  # set the index to the unique identifier\n",
    "    \n",
    "    for col in df2.select_dtypes(include=[np.number,np.datetime64]).columns:    \n",
    "        mean = df2[col].mean()\n",
    "        df2[col] = df2[col].fillna(mean) # fill missing values with mean         \n",
    "\n",
    "    # for col in df2.select_dtypes(include=[np.number,np.datetime64]).columns:   # apply filtering only to numerical columns\n",
    "    #     if col not in ['SalesID', 'SalePrice']:\n",
    "    #         mean = df2[col].mean()\n",
    "    #         std_dev = df2[col].std()\n",
    "    #         median = df2[col].median()\n",
    "    #         lower_bound = mean - 3 * std_dev\n",
    "    #         upper_bound = mean + 3 * std_dev\n",
    "\n",
    "    #         print(f\" percent of outliers in {col}: { ((df2[col] < lower_bound) | (df2[col] >= upper_bound)).sum()/df2[col].shape[0]*100:.2f},  min {df2[col].min():.2f}, max {df2[col].max():.2f} , mean {mean:.2f} , std {std_dev:.2f},  median {median:.2f}\")\n",
    "    #         # Normalize the values in place\n",
    "    #         df2[col] = df2[col].mask(((df2[col] < lower_bound) | (df2[col] > upper_bound)), median)    \n",
    "    # print (\"After 6 sigma filtering \", df2.shape )   \n",
    "        \n",
    "    return df2\n",
    "\n",
    "\n",
    "### MAIN #########\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "# Clean data\n",
    "print (\"Initial \", df.shape)\n",
    "print(\"-----------------------------------------------------\")\n",
    "df2 = manipulate_data(df)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print (\"After manipulate_data \", df2.shape )\n",
    "print(\"-----------------------------------------------------\")\n",
    "df2 = df2.drop_duplicates()  # drop duplicates \n",
    "print(\"After drop_duplicates \", df2.shape)\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "#print(\"before 6 sigma \", df2.shape )\n",
    "# #Remove outliers using 6 sigma method\n",
    "#for col in df2.select_dtypes(include=[np.number,np.datetime64]).columns:    \n",
    "#    mean = df2[col].mean()\n",
    "#    std_dev = df2[col].std()    \n",
    "#    lower_bound = mean - 3 * std_dev\n",
    "#    upper_bound = mean + 3 * std_dev\n",
    "#    df2 = df2[(df2[col] >= lower_bound) & (df2[col] <= upper_bound)]\n",
    "\n",
    "#print(\"After 6 sigma \", df2.shape )\n",
    "print(df2.head()  )  \n",
    "\n",
    "# Prepare data for training\n",
    "X = df2.drop(columns='SalePrice')\n",
    "y = df2['SalePrice']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols_For_onehot_encoding shortened Index(['UsageBand', 'ProductSize', 'ProductGroup', 'Drive_System', 'Enclosure',\n",
      "       'Forks', 'Pad_Type', 'Ride_Control', 'Stick', 'Transmission',\n",
      "       'Turbocharged', 'Blade_Extension', 'Blade_Width', 'Enclosure_Type',\n",
      "       'Engine_Horsepower', 'Hydraulics', 'Pushblock', 'Ripper', 'Scarifier',\n",
      "       'Tip_Control', 'Tire_Size', 'Coupler', 'Coupler_System',\n",
      "       'Grouser_Tracks', 'Hydraulics_Flow', 'Track_Type',\n",
      "       'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb', 'Pattern_Changer',\n",
      "       'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls',\n",
      "       'Differential_Type', 'Steering_Controls'],\n",
      "      dtype='object') 36\n",
      "Before===== (16000, 44) (4000, 52)\n",
      "After===== (16000, 151) (4000, 151)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16000 entries, 2285436 to 1644437\n",
      "Columns: 151 entries, ModelID to Steering_Controls_Four Wheel Standard\n",
      "dtypes: bool(143), float64(2), int32(2), int64(4)\n",
      "memory usage: 3.2 MB\n",
      " Best test RMSE: 10585.42 | y_test.std()=22837.32 | y_test.mean()=31125.87 \n",
      " Best train RMSE: 4237.40 | y_train.std()=23238.06 | y_train.mean()=31426.52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "MAX_DEPTH = 20\n",
    "ESTIMATORS=170\n",
    "TEST_SIZE=0.2\n",
    "\n",
    "###################################################\n",
    "def RMSE(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean() ** 0.5\n",
    "\n",
    "\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "X_small = X.sample(20000)\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "# X_small = X\n",
    "# y_small = y.loc[X_small.index]\n",
    "\n",
    "model = RandomForestRegressor(random_state=RANDOM_STATE, max_depth=MAX_DEPTH)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "# Dropping columns\n",
    "for col in X_train.columns:\n",
    "   if X_train[col].nunique() == 1:\n",
    "        print(\"Dropping \", col)\n",
    "        X_train.drop(col,inplace=True,axis=1)  \n",
    "\n",
    "# Perform one-hot encoding on the train dataset \n",
    "cols_For_onehot_encoding = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "#print(\"cols_For_onehot_encoding\",  cols_For_onehot_encoding , len(cols_For_onehot_encoding))\n",
    "to_drop = ['fiModelDesc','fiBaseModel','fiSecondaryDesc','fiModelSeries','fiModelDescriptor','fiProductClassDesc','ProductGroupDesc','state']\n",
    "\n",
    "for column in to_drop:\n",
    "    X_train = X_train.drop(column   , axis=1)    \n",
    "    cols_For_onehot_encoding = cols_For_onehot_encoding.drop(column)\n",
    "print(\"cols_For_onehot_encoding shortened\" ,cols_For_onehot_encoding, len(cols_For_onehot_encoding))\n",
    "\n",
    "print(\"Before=====\",  X_train.shape ,X_test.shape )\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=cols_For_onehot_encoding , drop_first=True)\n",
    "# Align the test dataset with the train dataset columns\n",
    "X_test = pd.get_dummies(X_test, columns=cols_For_onehot_encoding, drop_first=True)\n",
    "\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(\"After=====\",  X_train.shape ,X_test.shape )\n",
    "\n",
    "X_train.info()\n",
    "X_train.head()\n",
    "X_test.to_csv(f'X_test.csv')\n",
    "X_train.to_csv(f'X_train.csv')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "test_rmse = RMSE(y_test_pred, y_test)\n",
    "train_rmse = RMSE(y_train_pred, y_train)\n",
    "\n",
    "print(f\" Best test RMSE: {test_rmse:.2f} | y_test.std()={y_test.std():.2f} | y_test.mean()={y_test.mean():.2f} \")\n",
    "print(f\" Best train RMSE: {train_rmse:.2f} | y_train.std()={y_train.std():.2f} | y_train.mean()={y_train.mean():.2f}\")\n",
    "\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1) \n",
    "# feature_importances = pd.DataFrame(result.importances_mean, index=X_test.columns, columns=['Permutation Importance'])\n",
    "# feature_importances['Permutation Importance'] = feature_importances['Permutation Importance'] / feature_importances['Permutation Importance'].sum()\n",
    "# feature_importances['Feature importance'] = model.feature_importances_\n",
    "# feature_importances = feature_importances.sort_values(by='Permutation Importance', ascending=False)\n",
    "# print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial  (11573, 52)\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "After manipulate_data  (11573, 52)\n",
      "-----------------------------------------------------\n",
      "After one hot encoding and reindex  (11573, 151)\n",
      "-----------------------------------------------------\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 11573 entries, 1222837 to 6333349\n",
      "Series name: SalePrice\n",
      "Non-Null Count  Dtype  \n",
      "--------------  -----  \n",
      "11573 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 180.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set\n",
    "X_valid = pd.read_csv('valid.csv')\n",
    "print (\"Initial \", X_valid.shape)\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "X_valid = manipulate_data(X_valid)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print (\"After manipulate_data \", X_valid.shape )\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "# Align the test dataset with the train dataset columns\n",
    "#X_valid = pd.get_dummies(X_valid, columns=['datasource'], prefix='datasource' , drop_first=True)\n",
    "X_valid = pd.get_dummies(X_valid, columns=cols_For_onehot_encoding , drop_first=True)\n",
    "X_valid = X_valid.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print (\"After one hot encoding and reindex \", X_valid.shape )\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "y_valid_pred = model.predict(X_valid)  # predict on the validation set             \n",
    "y_valid_pred = pd.Series(y_valid_pred, index=X_valid.index, name='SalePrice')\n",
    "y_valid_pred.info()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "f'submission_{datetime.now().isoformat()}'\n",
    "y_valid_pred.to_csv(f'submission_{datetime.now().isoformat()}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# #print(df2.corr())\n",
    "# # Create a figure with a custom size\n",
    "# plt.figure(figsize=(12, 10))  # Adjust figsize to fit your needs\n",
    "# sns.heatmap(X_valid.corr(), annot=True, fmt='0.2f')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Predict and analyse the results\n",
    "# #Compute permutation feature importance\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ###################################################\n",
    "# def important_features_analysis(model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1):\n",
    "#     result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)    \n",
    "\n",
    "#     # Summarize feature importance    \n",
    "#     feature_importances = pd.DataFrame(result.importances_mean, index=X_test.columns, columns=['Permutation Importance'])\n",
    "#     feature_importances['Permutation Importance'] = feature_importances['Permutation Importance'] / feature_importances['Permutation Importance'].sum()\n",
    "#     feature_importances['Feature importance'] = model.feature_importances_\n",
    "#     feature_importances = feature_importances.sort_values(by='Permutation Importance', ascending=False)\n",
    "#     print(feature_importances)\n",
    "\n",
    "# ###################################################\n",
    "# def analyse_prediction(model , X_data, y_data , y_pred, random_state=RANDOM_STATE):\n",
    "    \n",
    "#     print(\"\\n\\nRMSE:\", round(RMSE(y_pred, y_data),2))\n",
    "#     print(\"STD\", round(y_data.std(),2))\n",
    "#     important_features_analysis(model, X_data, y_data)\n",
    "   \n",
    "\n",
    "\n",
    "# def plot_prediction(y_data, y_pred, title='Actual vs Predicted Prices'):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.scatterplot(x=y_data, y=y_pred)\n",
    "#     xx = np.linspace(y_data.min(), y_data.max(), 100)\n",
    "#     plt.plot(xx, xx, 'r--')\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('actual')\n",
    "#     plt.ylabel('predicted')\n",
    "#     plt.show()\n",
    "\n",
    "# ###################################################\n",
    "# def predict(model,X_data , y_data=None, random_state=RANDOM_STATE):\n",
    "#     y_pred = model.predict(X_data)\n",
    "#     if y_data is None:\n",
    "#         return y_pred\n",
    "    \n",
    "#     analyse_prediction(model, X_data , y_data , y_pred, random_state=random_state)    \n",
    "#     return y_pred\n",
    "# ###################################################\n",
    "# def summary(model,X_train, X_test, y_train, y_test, random_state=RANDOM_STATE):\n",
    "    \n",
    "#     y_train_pred = predict(model,X_train, y_train,random_state=random_state)\n",
    "#     y_test_pred = predict(model,X_test, y_test,random_state=random_state)   \n",
    "    \n",
    "#     plot_prediction(y_train, y_train_pred,\"Actual vs Predicted Prices (Train)\")\n",
    "     \n",
    "#     plot_prediction(y_test, y_test_pred,\"Actual vs Predicted Prices (Test)\")\n",
    "   \n",
    "#     return y_train_pred, y_test_pred\n",
    "\n",
    "# ###################################################\n",
    "# def summary2(model,X_train, X_test, y_train, y_test, random_state=RANDOM_STATE):\n",
    "    \n",
    "#     y_train_pred, y_test_pred = summary(model, X_train, X_test, y_train, y_test, random_state=random_state)\n",
    "#     return y_train_pred, y_test_pred\n",
    "    \n",
    "# ###################################################\n",
    "# def plot_residuals(y_true, y_pred, title):\n",
    "#     residuals = y_true - y_pred\n",
    "#     plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Predicted Values')\n",
    "#     plt.ylabel('Residuals')\n",
    "#     plt.axhline(y=0, color='r', linestyle='--')\n",
    "#     plt.show()\n",
    "\n",
    "# ###############################################################\n",
    "# # Assuming the summary function is defined as in the provided code snippet\n",
    "# def summary_with_residuals(model, X_train, X_test, y_train, y_test, random_state):\n",
    "#     y_train_pred, y_test_pred = summary(model, X_train, X_test, y_train, y_test, random_state=random_state)\n",
    "    \n",
    "#     # Plotting residuals for training data\n",
    "#     plot_residuals(y_train, y_train_pred, \"Residuals for Training Data\")\n",
    "    \n",
    "#     # Plotting residuals for test data\n",
    "#     plot_residuals(y_test, y_test_pred, \"Residuals for Test Data\")\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------------------------------------\n",
    "# # Call the modified summary function with residuals plotting\n",
    "# summary_with_residuals(model, X_train, X_test, y_train, y_test, random_state=RANDOM_STATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
